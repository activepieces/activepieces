{
  "Extract structured data from websites using AI with natural language prompts": "使用自然语言提示从网站提取结构化数据",
  "\nFollow these steps to obtain your Firecrawl API Key:\n\n1. Visit [Firecrawl](https://firecrawl.dev) and create an account.\n2. Log in and navigate to your dashboard.\n3. Locate and copy your API key from the API settings section.\n": "\nFollow these steps to obtain your Firecrawl API Key:\n\n1. Visit [Firecrawl](https://firecrawl.dev) and create an account.\n2. Log in and navigate to your dashboard.\n3. Locate and copy your API key from the API settings section.\n",
  "Scrape Website": "查询网站",
  "Start Crawl": "启动 Crawl",
  "Crawl Results": "Crawl 结果",
  "Custom API Call": "自定义 API 呼叫",
  "Scrape a website by performing a series of actions like clicking, typing, taking screenshots, and extracting data.": "通过点击、输入、截图和提取数据等一系列操作来扫描一个网站。",
  "Start crawling multiple pages from a website based on specified rules and patterns.": "根据指定的规则和模式，开始从网站上抽取多个页面。",
  "Get the results of a crawl job.": "获取一个 crawl 作业的结果。",
  "Make a custom API call to a specific endpoint": "将一个自定义 API 调用到一个特定的终点",
  "Website URL": "网站 URL",
  "Timeout (ms)": "超时(ms)",
  "Perform Actions Before Scraping": "在拆解前执行操作",
  "Action Properties": "动作属性",
  "Extraction Type": "提取类型",
  "Extraction Properties": "提取属性",
  "URL": "网址",
  "Exclude Paths": "排除路径",
  "Include Paths": "包含路径",
  "Maximum Path Depth": "最大路径深度",
  "Maximum Discovery Depth": "最大发现深度",
  "Ignore Sitemap": "Ignore Sitemap",
  "Ignore Query Parameters": "忽略查询参数",
  "Limit": "限制",
  "Allow Backward Links": "允许后向链接",
  "Allow External Links": "允许外部链接",
  "Deliver Results to Webhook": "将结果投递到 Webhook",
  "Webhook Properties": "Webhook 属性",
  "Crawl ID": "Crawl ID",
  "Method": "方法",
  "Headers": "信头",
  "Query Parameters": "查询参数",
  "Body": "正文内容",
  "Response is Binary ?": "响应是二进制的？",
  "No Error on Failure": "失败时没有错误",
  "Timeout (in seconds)": "超时(秒)",
  "The webpage URL to scrape.": "要扫描的网页 URL。",
  "Maximum time to wait for the page to load (in milliseconds).": "等待页面加载的最大时间 (毫秒)。",
  "Enable to perform a sequence of actions on the page before scraping (like clicking buttons, filling forms, etc.). See [Firecrawl Actions Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions) for details on available actions and their parameters.": "启用在拆解前在页面上执行一系列动作(如点击按钮、填充表单等)。 关于可用动作及其参数的详细信息，请参阅[Firecrawl Actions Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions)。",
  "Properties for actions that will be performed on the page.": "将在页面上执行的操作的属性。",
  "Choose how to extract data from the webpage.": "选择如何从网页中提取数据。",
  "Properties for data extraction from the webpage.": "从网页提取数据的属性。",
  "The base URL to start crawling from.": "要开始卷起的基础URL。",
  "URL pathname regex patterns that exclude matching URLs from the crawl. For example, if you set \"excludePaths\": [\"blog/.*\"] for the base URL firecrawl.dev, any results matching that pattern will be excluded, such as https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap.": "URL pathname 正则表达式排除与 crawl 匹配的 URL。例如，如果您设置了 \"exclusidePaths\"：[\"blog/.*\"] 基本的 URL firecrawl。 任何匹配此模式的结果都将被排除，例如https://www.fireecrawl.dev/blog/fireecrawl-launch-week-1-recap。",
  "URL pathname regex patterns that include matching URLs in the crawl. Only the paths that match the specified patterns will be included in the response. For example, if you set \"includePaths\": [\"blog/.*\"] for the base URL firecrawl.dev, only results matching that pattern will be included, such as https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap.": "包含在 crawl 中匹配的 URL 路径名正则表达式模式。 只有匹配指定模式的路径将被包含在响应中。例如，如果您设置了“inclusdePaths”：“博客/”。 “] 对于基本的 URL fireecrawl.dev，只有与该模式相匹配的结果才会被包括在内，如https://www.fireecrawl.dev/blog/fireecrawl-launch-week-1-recap。",
  "Maximum depth to crawl relative to the base URL. Basically, the max number of slashes the pathname of a scraped URL may contain.": "相对于基础URL的最大深度。基本上，废料的 URL 路径名称可能包含的最大斜线数。",
  "Maximum depth to crawl based on discovery order. The root site and sitemapped pages has a discovery depth of 0. For example, if you set it to 1, and you set ignoreSitemap, you will only crawl the entered URL and all URLs that are linked on that page.": "基于发现顺序的 crawl 的最大深度。根站点和站点页面发现深度为 0。 例如，如果您设置为 1，您设置了 ignoreSitemap 您只会将输入的 URL 和所有链接在该页面上的URL变砖。",
  "Ignore the website sitemap when crawling": "爬行时忽略站点地图",
  "Do not re-scrape the same path with different (or none) query parameters": "不要用不同(或非)查询参数重新扫描相同的路径",
  "Maximum number of pages to crawl. Default limit is 10000.": "最大页面数到 crawl。默认限制为 10000。",
  "Enables the crawler to navigate from a specific URL to previously linked pages.": "启用 crawler 来导航特定的 URL 到以前链接的页面。",
  "Allows the crawler to follow links to external websites.": "允许crawler跟踪外部网站链接。",
  "Enable to send crawl results to a webhook URL.": "启用可将 crawl 结果发送到Webhook URL。",
  "Properties for webhook configuration.": "Webhook 配置的属性。",
  "The ID of the crawl job to check.": "要检查的 crawl 作业的 ID。",
  "Authorization headers are injected automatically from your connection.": "授权头自动从您的连接中注入。",
  "Enable for files like PDFs, images, etc..": "启用 PDF、图像等文件。",
  "GET": "获取",
  "POST": "帖子",
  "PATCH": "PATCH",
  "PUT": "弹出",
  "DELETE": "删除",
  "HEAD": "黑色"
}