{
  "Extract structured data from websites using AI with natural language prompts": "自然言語プロンプトでAIを使用してウェブサイトから構造化データを抽出する",
  "\nFollow these steps to obtain your Firecrawl API Key:\n\n1. Visit [Firecrawl](https://firecrawl.dev) and create an account.\n2. Log in and navigate to your dashboard.\n3. Locate and copy your API key from the API settings section.\n": "\nFollow these steps to obtain your Firecrawl API Key:\n\n1. Visit [Firecrawl](https://firecrawl.dev) and create an account.\n2. Log in and navigate to your dashboard.\n3. Locate and copy your API key from the API settings section.\n",
  "Scrape Website": "ウェブサイトをスクレイプ",
  "Extract Structured Data": "構造化データの抽出",
  "Crawl": "Crawl",
  "Crawl Results": "Crawl の結果",
  "Map Websites": "ウェブサイトのマップ",
  "Custom API Call": "カスタムAPI通話",
  "Scrape a website by performing a series of actions like clicking, typing, taking screenshots, and extracting data.": "クリック、入力、スクリーンショットの取得、データの抽出など、一連のアクションを実行してウェブサイトをスクラップします。",
  "Extract structured data from multiple URLs using AI.": "AIを使用して複数のURLから構造化データを抽出します。",
  "Crawl multiple pages from a website based on specified rules and patterns.": "指定されたルールとパターンに基づいてウェブサイトから複数のページをクロールします。",
  "Get the results of a crawl job.": "クロールジョブの結果を取得します。",
  "Input a website and get all the urls on the website.": "ウェブサイトを入力し、ウェブサイト上のすべてのURLを取得します。",
  "Make a custom API call to a specific endpoint": "特定のエンドポイントへのカスタム API コールを実行します。",
  "Website URL": "Website URL",
  "Timeout (ms)": "タイムアウト (ミリ秒)",
  "Perform Actions Before Scraping": "スクラップする前にアクションを実行",
  "Action Properties": "アクションのプロパティ",
  "Output Format": "出力形式",
  "Extraction Prompt": "Extraction Prompt",
  "Schema Mode": "スキーマモード",
  "Data Definition": "データ定義",
  "URLs": "URL",
  "Enable Web Search": "ウェブ検索を有効にする",
  "Timeout (seconds)": "タイムアウト (秒)",
  "Data Schema Type": "データスキーマタイプ",
  "URL": "URL",
  "Prompt": "Prompt",
  "Limit": "制限",
  "Only Main Content": "メインコンテンツのみ",
  "Deliver Results to Webhook": "Webhookに結果を配信",
  "Webhook Properties": "Webhook プロパティ",
  "Crawl ID": "クロールID",
  "Main Website URL": "ウェブサイトのメインURL",
  "Include subdomain": "サブドメインを含める",
  "Method": "方法",
  "Headers": "ヘッダー",
  "Query Parameters": "クエリパラメータ",
  "Body": "本文",
  "Response is Binary ?": "応答はバイナリですか?",
  "No Error on Failure": "失敗時にエラーはありません",
  "Timeout (in seconds)": "タイムアウト（秒）",
  "The webpage URL to scrape.": "スクレイプするウェブページのURL。",
  "Maximum time to wait for the page to load (in milliseconds).": "ページがロードされるまでの最大時間 (ミリ秒単位)。",
  "Enable to perform a sequence of actions on the page before scraping (like clicking buttons, filling forms, etc.). See [Firecrawl Actions Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions) for details on available actions and their parameters.": "ページをスクレイピングする前に一連のアクションを実行する (クリックボタン、フォームの入力など) を有効にします。 利用可能なアクションとパラメータの詳細については、[Firerawl Actions Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions)を参照してください。",
  "Properties for actions that will be performed on the page.": "ページ上で実行されるアクションのプロパティ。",
  "Choose what format you want your output in.": "出力するフォーマットを選択します。",
  "Prompt for extracting data.": "データの抽出を促します。",
  "Data schema type.": "データスキーマ型。",
  "Add one or more URLs to extract data from.": "データを抽出するために 1 つ以上の URL を追加します。",
  "Describe what information you want to extract.": "抽出したい情報を記述してください。",
  "Enable web search to find additional context.": "追加のコンテキストを見つけるためにWeb検索を有効にします。",
  "Timeout in seconds after which the task will be cancelled": "タスクがキャンセルされるまで数秒後にタイムアウトします",
  "For complex schema, you can use advanced mode.": "複雑なスキーマでは、高度なモードを使用できます。",
  "The base URL to start crawling from.": "クロールを開始するベースURL。",
  "Maximum number of pages to crawl. Default limit is 10.": "クロールするページの最大数。デフォルトの制限は 10 です。",
  "Only return the main content of the page, excluding headers, navs, footers, etc.": "ヘッダー、ナビ、フッターなどを除いたページのメインコンテンツのみを返します。",
  "Enable to send crawl results to a webhook URL.": "クロール結果をWebhookのURLに送信するよう有効にします。",
  "Properties for webhook configuration.": "Webhook設定のプロパティです。",
  "The ID of the crawl job to check.": "チェックするクロールジョブのID。",
  "The webpage URL to start scraping from.": "スクレイピングを開始するウェブページのURL。",
  "Include and crawl pages from subdomains of the target website (e.g., blog.example.com, shop.example.com) in addition to the main domain.": "メインドメインに加えて、対象のウェブサイトのサブドメイン(blog.example.com、shop.example.comなど)からページを含めてクロールします。",
  "Maximum number of links to return (max: 100,000)": "リターンするリンクの最大数（最大：100,000）",
  "Authorization headers are injected automatically from your connection.": "認証ヘッダは接続から自動的に注入されます。",
  "Enable for files like PDFs, images, etc..": "PDF、画像などのファイルを有効にします。",
  "Simple": "単純な",
  "Advanced": "高度な設定",
  "GET": "取得",
  "POST": "POST",
  "PATCH": "PATCH",
  "PUT": "PUT",
  "DELETE": "削除",
  "HEAD": "頭"
}