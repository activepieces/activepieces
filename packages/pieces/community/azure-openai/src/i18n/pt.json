{
  "Powerful AI tools from Microsoft": "Ferramentas poderosas de IA da Microsoft",
  "Endpoint": "Endpoint",
  "API Key": "Chave de API",
  "https://<resource name>.openai.azure.com/": "https://<resource name>.openai.azure.com/",
  "Use the Azure Portal to browse to your OpenAI resource and retrieve an API key": "Use o Azure Portal para navegar até seu recurso OpenAI e recuperar uma chave de API",
  "Ask GPT": "Perguntar GPT",
  "Ask ChatGPT anything you want!": "Pergunte ao ChatGPT o que você quiser!",
  "Deployment Name": "Nome de implantação",
  "Question": "Questão",
  "Temperature": "Temperatura",
  "Maximum Tokens": "Máximo de Tokens",
  "Top P": "Superior P",
  "Frequency penalty": "Penalidade de frequência",
  "Presence penalty": "Penalidade de presença",
  "Memory Key": "Chave de memória",
  "Roles": "Papéis",
  "The name of your model deployment.": "O nome do seu modelo de implantação.",
  "Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.": "Controla aleatoriedade: Diminuir resulta em menos complementos aleatórios. À medida que a temperatura se aproxima de zero, o modelo se tornará determinístico e repetitivo.",
  "The maximum number of tokens to generate. Requests can use up to 2,048 or 4,096 tokens shared between prompt and completion depending on the model. Don't set the value to maximum and leave some tokens for the input. (One token is roughly 4 characters for normal English text)": "O número máximo de tokens a gerar. Solicitações podem usar até 2.048 ou 4,096 tokens compartilhados entre prompt e conclusão dependendo do modelo. Não defina o valor como máximo e deixe alguns tokens para o input. (Um token é aproximadamente 4 caracteres para o texto normal em inglês)",
  "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.": "Uma alternativa à amostragem com temperatura, chamada amostragem núcleo, onde o modelo considera os resultados dos tokens com massa de probabilidade superior (P). Portanto, 0,1 significa que apenas os tokens que incluem a massa de probabilidade superior de 10% são considerados.",
  "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.": "Número entre -2.0 e 2.0. Valores positivos penalizam novos tokens baseados em sua frequência existente no texto até agora, diminuindo a probabilidade do modelo repetir o verbal da mesma linha.",
  "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the mode's likelihood to talk about new topics.": "Número entre -2.0 e 2.0. Valores positivos penalizam novos tokens baseado no fato de eles aparecerem no texto até agora, aumentando a probabilidade de o modo falar sobre novos tópicos.",
  "A memory key that will keep the chat history shared across runs and flows. Keep it empty to leave ChatGPT without memory of previous messages.": "Uma chave de memória que manterá o histórico de bate-papo compartilhado entre execuções e fluxos. Deixe em branco para deixar o ChatGPT sem memória das mensagens anteriores.",
  "Array of roles to specify more accurate response": "Array de papéis para especificar uma resposta mais precisa"
}